<div align="center">

# ğŸ›¡ï¸ MIST-VLA

### **M**echanistic **I**nterpretability for **S**afer **T**argeted Steering in **V**ision-**L**anguage-**A**ction Models

[![Python 3.8+](https://img.shields.io/badge/python-3.8+-blue.svg)](https://www.python.org/downloads/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![Code style: black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/psf/black)

*Per-dimension collision forecasting and targeted activation steering for safer Vision-Language-Action models on LIBERO*

[Key Features](#-key-features) â€¢
[Installation](#-installation) â€¢
[Quick Start](#-quick-start) â€¢
[Documentation](#-project-structure) â€¢
[Citation](#-citation)

</div>

---

## ğŸ“‹ Overview

MIST-VLA is a research framework for improving the safety and reliability of Vision-Language-Action (VLA) models through mechanistic interpretability and targeted activation steering. This project focuses on:

- **Collision Forecasting**: Per-dimension risk prediction for robotic manipulation tasks
- **Activation Steering**: Targeted interventions to mitigate unsafe behaviors
- **Failure Analysis**: SAFE-style labeling and comprehensive failure characterization
- **Empirical Evaluation**: Rigorous testing on LIBERO benchmark tasks

## âœ¨ Key Features

- ğŸ¯ **Per-Dimension Risk Prediction**: Train predictors for fine-grained collision forecasting
- ğŸ”„ **Data Collection Pipeline**: Automated rollout collection with internal signal logging
- ğŸ§  **Activation Steering**: Extract and apply steering vectors for targeted safety interventions
- ğŸ“Š **Comprehensive Evaluation**: Success rate, collision metrics, and recovery rate analysis
- ğŸ”§ **Modular Design**: Clean interfaces for model wrappers, data collection, and evaluation
- ğŸš€ **Production Ready**: Supports both local development and HPC cluster deployments

## ğŸ—‚ï¸ Project Structure

```
MIST-VLA/
â”œâ”€â”€ mist-vla/              # Main package
â”‚   â”œâ”€â”€ scripts/           # Runnable entry points
â”‚   â”œâ”€â”€ src/               # Core implementation
â”‚   â”‚   â”œâ”€â”€ data/         # Data collection and processing
â”‚   â”‚   â”œâ”€â”€ models/       # Model wrappers and interfaces
â”‚   â”‚   â”œâ”€â”€ steering/     # Activation steering implementation
â”‚   â”‚   â””â”€â”€ evaluation/   # Evaluation metrics and utilities
â”‚   â”œâ”€â”€ configs/          # Configuration files
â”‚   â”œâ”€â”€ requirements.txt  # Python dependencies
â”‚   â””â”€â”€ setup.py         # Package installation
â”œâ”€â”€ FailSafe_code/        # FailSafe baseline implementation
â”œâ”€â”€ LIBERO/              # LIBERO benchmark environment
â”œâ”€â”€ openvla-oft/         # OpenVLA-OFT evaluation pipeline
â””â”€â”€ README.md            # This file
```

## ğŸš€ Installation

### Prerequisites

- Python 3.8 or higher
- CUDA-capable GPU (recommended for training and large-scale rollouts)
- LIBERO environment properly configured

### Setup

1. **Clone the repository**
```bash
git clone https://github.com/yourusername/MIST-VLA.git
cd MIST-VLA
```

2. **Install dependencies**
```bash
cd mist-vla
pip install -r requirements.txt
pip install -e .
```

3. **Configure LIBERO**
Ensure LIBERO config exists at `~/.libero/config.yaml` with valid paths.

4. **Set up Python path**
```bash
export PYTHONPATH=$PWD:../openvla-oft:$PYTHONPATH
```

## ğŸ¬ Quick Start

### Collect Failure Data

```bash
cd mist-vla
python scripts/collect_failure_data_oft_eval.py \
  --env libero_spatial \
  --model-name moojink/openvla-7b-oft-finetuned-libero-spatial \
  --n_success 10 \
  --n_failure 10 \
  --max-attempts-per-task 5 \
  --camera-res 256 \
  --save_dir data/rollouts_oft_eval \
  --seed 0
```

### Train Risk Predictor

```bash
python scripts/train_risk_predictor.py \
  --data_dir data/rollouts_oft_eval \
  --output_dir experiments/risk_predictor \
  --epochs 50
```

### Extract Steering Vectors

```bash
python scripts/extract_steering_vectors.py \
  --data_dir data/rollouts_oft_eval \
  --model_name moojink/openvla-7b-oft-finetuned-libero-spatial \
  --output_dir data/steering_vectors
```

### Run Evaluation

```bash
python scripts/run_evaluation.py \
  --env libero_spatial \
  --model_name moojink/openvla-7b-oft-finetuned-libero-spatial \
  --steering_vectors data/steering_vectors \
  --n_episodes 100
```

## ğŸ“– Documentation

Comprehensive documentation is available in the [`docs/`](docs/) directory:

- [Getting Started Guide](docs/GETTING_STARTED.md) - Detailed installation and setup
- [Architecture Overview](docs/ARCHITECTURE.md) - System design and components
- [API Reference](docs/API.md) - Complete API documentation
- [FAQ](docs/FAQ.md) - Frequently asked questions
- [Dependencies](DEPENDENCIES.md) - External dependencies guide
- [Changelog](CHANGELOG.md) - Version history

## ğŸ“š Key Scripts

| Script | Description |
|--------|-------------|
| `collect_failure_data_oft_eval.py` | Uses OpenVLA-OFT eval pipeline and logs MIST-VLA signals (actions, hidden states, collisions, robot states) |
| `collect_failure_data.py` | Custom data collector with optional perturbation support |
| `collect_phase1_data.py` | Phase 1 data collection with collision labels |
| `train_risk_predictor.py` | Train per-dimension failure predictor |
| `extract_steering_vectors.py` | Build steering vectors for targeted mitigation |
| `run_evaluation.py` | Evaluate success rate, collisions, and recovery metrics |

## âš™ï¸ Configuration

Key configuration options can be found in `mist-vla/configs/`. Customize:
- Model parameters
- Data collection settings
- Training hyperparameters
- Evaluation metrics

## ğŸ”¬ Research

This project explores mechanistic interpretability techniques applied to VLA models to improve safety in robotic manipulation. Key research directions include:

- Understanding failure modes in VLA models
- Developing targeted interventions without full model retraining
- Scaling interpretability techniques to large vision-language-action models

## ğŸ“ Notes

- LIBERO requires a config file at `~/.libero/config.yaml` with valid asset and dataset paths
- For OpenVLA-OFT integration, ensure `openvla-oft` is on your `PYTHONPATH`
- GPU acceleration is strongly recommended for large-scale rollouts and training
- All experiments are reproducible with fixed random seeds

## ğŸ¤ Contributing

Contributions are welcome! Please see [CONTRIBUTING.md](CONTRIBUTING.md) for guidelines.

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## ğŸ“– Citation

If you use this code in your research, please cite:

```bibtex
@software{mist-vla2025,
  title={MIST-VLA: Mechanistic Interpretability for Safer Targeted Steering in Vision-Language-Action Models},
  author={Your Name},
  year={2025},
  url={https://github.com/yourusername/MIST-VLA}
}
```

## ğŸ™ Acknowledgments

- [LIBERO](https://libero-project.github.io/) - Benchmark environment
- [OpenVLA](https://openvla.github.io/) - Vision-Language-Action model
- FailSafe - Baseline safety methods

---

<div align="center">
Made with â¤ï¸ for safer robotics
</div>
