\section{Related Work}
\paragraph{Foundation robot policies and VLAs.}
Large VLA systems enable broad language-conditioned behavior but frequently require additional deployment-time stabilization. We position our method as an inference-time safety adapter for this regime.

\paragraph{Imitation policy baselines.}
ACT and Diffusion Policy represent strong task-specialized baselines. Evaluating a shared safety interface on both VLA and imitation policies is central to this work's cross-architecture scope.

\paragraph{Failure detection versus continuous recovery.}
A key distinction in this paper is between (i) predicting failure and stopping, and (ii) predicting a directional correction for recovery. Our latent-stop baseline isolates detection-only behavior, while steering evaluates continuous intervention.

\paragraph{Planning-based controllers.}
Sampling-based planners such as MPPI provide robust optimization baselines but can be computationally expensive at high control frequency. We compare directly against MPPI and report per-step controller latency.

\paragraph{Training-time safety constraints versus inference-time adaptation.}
Constraint-in-training approaches can improve safety but are model-specific and expensive to retrain. Our approach is complementary: frozen-policy adaptation at inference-time via hidden-state probing.

\paragraph{Placeholder citations to finalize.}
TODO: SAFE citation], TODO: SafeVLA citation], TODO: OpenVLA citation], TODO: Octo citation], TODO: ACT citation], TODO: Diffusion Policy citation], TODO: MPPI citation], TODO: LIBERO citation].
