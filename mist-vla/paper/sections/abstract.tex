\begin{abstract}
We study whether a lightweight safety controller can attach to the latent states of large robot policies and improve failure recovery without retraining the base policy. Our setting targets both Vision-Language-Action (VLA) foundation models and non-VLA policies (e.g., ACT), with a single controller interface. We train a three-head multilayer perceptron (MLP) that predicts (i) imminent failure probability, (ii) time-to-failure, and (iii) Cartesian correction $\Delta \mathbf{p}=(\Delta x,\Delta y,\Delta z)$ from hidden states only. The correction is deployed via a gated controller with magnitude clamp and optional fail-probability gating.

Empirically, we evaluate against vanilla policy execution, random noise, EMA-only smoothing, random latent jiggle, and MPPI. Our current completed paired runs show near-parity versus MPPI across ACT and OpenVLA, with specific configurations exceeding MPPI in cross-model mean performance. We also measure controller latency and show that the latent steering apply step is substantially faster than MPPI in our benchmark ($0.628$ ms mean vs $4.064$ ms mean; $6.47\times$ speedup). These results indicate that latent safety steering is a practical, low-overhead intervention mechanism that can generalize across policy families, while highlighting remaining calibration challenges for robust cross-model superiority.
\end{abstract}
